\chapter{Team 1 Agent Design}\label{team_1_agent_design}

\section{Overview}

The following chapter presents the design and implementation of Team 1's agent. The agent presented agent seeks to imitate important parts of human behaviour such as social status, social networks, selfishness, forgiveness and learning from experiences. At the end of the chapter, it will be shown that by using these concepts, the agent produces better collective outcomes over agents performing random actions.

\section{Core Structure}
Figure \ref{fig:agent_structure} presents a high-level overview of the structure and operation of our agent. In broad terms, the agent contains three elements which impact decision-making. These elements are:
\begin{enumerate}
  \item Social capital: Socially constructed resources which facilitate cooperation to solve collective problems.
  \item Selfishness: A continuously updated variable dictating to what degree an  agent prioritizes its own utility over the social welfare of the group when making decisions. 
  \item Q-functions: Functions learned through reinforcement learning which estimate the impact of actions given the current game state.
\end{enumerate}
In addition to these components which are inherent to the agent, the current state of the game, known as the \emph{environment}, also influences decisions. Exactly how each component is used in the decision making process varies between different types of decisions, and social capital, selfishness and q-functions are not all used for every decision. The specific decision process for each type of decision will be discussed in greater detail in \ref{}.

%However, a general decision flow can be as follows: The agent takes in the current game state and passes it through a set of Q-functions. The Q-functions return the expected utility of each action to the collective and to the agent itself. Based on its current selfishness value, the agent aggregates the two utility values into a single value for each action.   Generally, the decision flow 

After each round of the game, the agents will update their internal state. Specifically, the agents will update their selfishness based on the current environment and the social capital of agents, while the social capital will be updated based on the actions of agents. The social capital is also updated every time an agent receives a message from an agent in their network. All of these update mechanisms are indicated by red arrows in Figure \ref{fig:agent_structure}. As can be seen, there is no arrow feeding back to the Q-functions as they do not update over the course of a single game. Instead, the Q-functions are updated based on the results from a series of games. However, as will be discussed later, the updating of selfishness values does still permit the agents to learn from experiences during a game.

In addition to selfishness and social capital the agent also maintains a set of state variables that are used to make decisions with regard to leadership and voting. The most important of these is a measure of how well each agent in currently doing. This variable has to take into account many different aspects of game state and can be fairly difficult to create as defining a good game position is not obvious and of course depends on the decisions of others. An example of this difficulty is in deciding whether or not an agent having a higher attack value suggests it is doing better, in this case we found this to be a bad thing as in makes the agent more likely to have to engage in fights compared to agents with lower attacks. Several different factors such as this were combined to form this single variable which will be referred to as agent survival likelihood from here on. Once this score was computed for each agent all the scores were scaled such that the maximum score was less than 1 and the minimum was greater that 0, in addition, the mean was shifted so as to be close to 0.5. Finally, the standard deviation of this data-set was computed to give an indication of the spread of agents states, as the variation in state across the population can be a big factor in agent decision making. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{004_team_1_agent_design/images/agent_structure.png}
    \caption{Overview of agent structure.}
    \label{fig:agent_structure}
\end{figure}

\section{Social Capital}

The core concept underlying the design of the agent is the idea of social capital. The exact definition of the term "social capital" varies between works, but can be summarized as "an 'umbrella term' for a range of socially constructed conceptual resources that help people coordinate expectations and self-organise."\cite{pitt}. Several forms of social capital have been identified and defined. Our work on social capital is mostly related to those forms laid out by Elinor Ostrom and T. K. Ahn in their 2007 paper "The meaning of social capital and its link to collective action". \cite{ostrom-ahn} In their paper, Ostrom and Ahn identified three forms of social capital. These were \emph{Institutions}, \emph{Networks} and \emph{Trustworthiness}. In addition to the forms of social capital presented by Ostrom and Ahn, we have as a team identified another form of social capital elected to call \emph{Honour}. With \emph{Honour} we refer to the human tendency to want to return a favour, or similarly, our appetite for revenge. A more comprehensive definition of honour is given in section \ref{subsection:honour}. 

In order to use social capital to promote cooperation, it must be used as part of a framework where the actions of an agent impact their social capital and where the social capital is used to make informed decision on whether or not to cooperate with other agents. For our agent, we used a similar framework to that presented by Petruzzi, Pitt, and Busquets\cite{complexity_reduction}. An overview diagram of this framework is presented in Figure \ref{fig:social_capital_framework}. In this framework events coming the environment are translated into social capital information which is passed to internal update functions for calculating metrics for each type of social capital. The social capital decision module uses the social capital metrics to calculate a value from 0 (no cooperation) to 1 (full cooperation) indicating whether an agent should cooperate or not. Implementing this framework for a simple cooperation game, Petruzzi et al. were able to achieve better performance than the dominant strategy given by game theory analysis. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{004_team_1_agent_design/images/socialcapitalframework.png}
    \caption{Framework for a social capital system.\cite{pitt}}
    \label{fig:social_capital_framework}
\end{figure}

To apply the framework to the Escape the Dark Pit(t) game, we used a slightly different implementation than Petruzzi et al. used for their cooperation game. Most prominently, we simplified the stored metrics on social capital down to a single number between -1 and 1 for each form of social capital. We also simplified the social capital decision module by removing the possibility of weighting social capital indicators differently, reducing the decision module to a simple summing function. 

The specific implementation is as follows. Each agent has a member variable $socialCaptial$ which is a map between agent ids and an array with 4 values. From index 0 to index 3, this array holds values describing the social capital related to institutions, networks, trustworthiness and honour respectively for the agent with the specified agent id. Each agents $socialCapital$ map contains an array for every single agent in the game, including themselves. The array values are bounded between -1 and 1, where a higher number indicates higher social capital. When an agent is created, all the array values are initialized as 0. This is a neutral state in which no agent has a negative nor positive impression of another agent. Throughout the game the array values for each agent will change based on their actions and messages received about them. In general, if an agent does a non-cooperative actions the array values will decrease and become negative, and inversely cooperative actions will give positive array values. As such, agents will try to reward other agents for which their $socialCapital$ map indicates positive social capital and punish those who have negative social capital.

The following subsections go into detail regarding the implementation of each form of social capital.

\subsection{Institutions}

In their 2007 paper Ostrom and Ahn defined institutions as "prescriptions that specify what actions (or outcomes) are required, prohibited, or permitted, and the sanctions authorized if the rules are not followed" \cite{ostrom-ahn}. With our $Institutions$ social capital value we seek to provide a metric on how likely an agent is to follow these prescriptions and cooperate within institutions. 

During a game our agents place the following expectation on the interaction of other agents with institutions:

\begin{enumerate}
    \item If the leader makes a fight proposal, all agents should follow that proposal.
    \item If the leader makes a loot allocation proposal, all agents should follow that proposal.
\end{enumerate}

Failing to comply with any of these two expectations will lead to an agent being labelled as a defector. Once labelled as a defector, an agent's social capital value for institutions will for the rest of the game be given a base value of -1. As such, even deviating from the required action once will lead to a negative perception for the entirety of the remaining game. This is similar to a grim trigger strategy for cooperation. The reason why we elected to have defecting in even a single round negatively effect the social capital of an agent for all remaining rounds is due to the limitation on available information from the game. In its current version, the list of defectors which the game makes available to agent has the same type of static behaviour where defecting once will lead to an agent being labelled a defector for the rest of the game.

In order to increase their $Institutions$ social capital value agents must demonstrate their cooperation within institutions. For our agents there is just a single way of doing this: being elected leader. For as long as an agent is the leader, they will get a temporary boost of +1 to their $Institutions$ social capital value. Originally, agents tendency to sanction defectors was also planned to impact their social capital. In this case, doing actions which punish defectors such as voting against them in elections and not trading with them would yield positive social capital. Inversely, voting for defectors or trading with them would yield negative social capital. However, this was in practice found to be hard to implement, and has been left as a possible future extension.

\subsection{Networks}

The $Networks$ social capital value is a measure of an agent's reputation among other agents. Specifically, the $Networks$ value tries to estimate the average social capital perception of the given agent among the other agents in the game. 

 !!!!
SHERWIN here you need to talk about:
- How an agents $Networks$ value is updated
- What messages we send between agents
- When we send messages between agents
- The connections between agents
 !!!

\subsection{Trustworthiness}

In their 2007 paper Ostrom and Ahn give multiple examples of trustworthiness, but give no concise definition. One definition fitting the examples of Ostrom and Ahn is that given by Pitt in his 2021 book on Self-Organising Multi-Agent Systems: "Trustworthiness, as distinct from trust, and related to reputation, being a shared understanding of someoneâ€™s willingness to honour agreements and commitments"\cite{pitt}. In their paper, Ostrom and Ahn treat trustworthiness as a function of an individuals observable characteristics such as appearance, dress, gender, age and language. Moreover, they give examples of so called "one-shot" situations in which an individual has to decide on whether to trust another individual which they have never previously interacted with.\cite{ostrom-ahn}

Given that agents' interactions in the Escaping the Dark Pit(t) game are not one-shot, but repeated, handling trustworthiness as an inherent property of an agent's current observable characteristic is not ideal. Instead, as agents interact many times, it is more natural to use previous interactions to estimate future cooperativeness. Our agent therefore does not use observable characteristics to determine trustworthiness, as Ostrom and Ahn did. Instead, we define an agent's trustworthiness in the following way: "Trustworthiness is an agents tendency to cooperate with and trust other agents as evidenced by their past actions."

Each agent updates the $Trustworthiness$ social capital value of other agents at the end of the round in the following way: The agent takes in the other agent's current state and passes it through its Q-functions. From the Q-functions then give the potential social welfare of each of the agent's possible actions. Depending on their utility to the group, each action is placed on a scale from -1 to 1. Where the value of 1 is given to the most beneficial action and -1 is given to the least beneficial action. Actions which are neither the best nor the worst are assigned a value by interpolating between -1 and 1 based on the social welfare of the action. With every action assigned a cooperativeness value between -1 and 1, the agent now reads in the selected action of the other agent. 

\subsection{Honour}
\label{subsection:honour}

\section{Social Bias}

If the aim of the entire game from the point of view of each agent was to get as many agents total to escape the pit without regard for whether they themselves make it out all agents would simply implement whatever the optimum strategy is to maximise total agent survival, (although this strategy can be relatively difficult to find). However, since every agent also wants to ensure it is one of the surviving agents the game becomes a  competition to ensure a place amongst this surviving group. This concept introduces the idea of punishing agents with bad social capital, resulting in agents attempting to see how much selfish behaviour they can get away with before the negative social consequences out way the potential gain. Thus in order to incorporate this logic into our agents decision making it is important to not only maintain a score of social capital of other agents but also an estimation of what the collectives groups opinion is of our agent. This value can be fed into our decision making to adjust the selfishness and cooperation of our agent if it finds that the group has a bias against it. 

In order to measure the social bias against our agent at each round we try and estimate whether or not we are being punished. What constitutes a punishment can again be very difficult to define as this requires knowledge of what a good game position is, which in turn requires knowledge of the optimum strategy. To simplify this problem our agent assumes that being made to fight is generally a bad thing as this can lead to loss of health, therefore, we need to try and keep track of whether or not our agent is made to fight more often than it should. The algorithm devised for this makes use of the survival likelihood scores mentioned earlier to find a set of agents in a similar position to our own agent, using the standard deviation of this score to adjust the threshold of similarity. Once the set of agents similar to our own has been found we can then find the fraction of this set of agents that were made to fight and also the boolean value of whether our agent is made to fight. What we are trying to measure using these two variables is whether there is a statistical difference in our fight frequency compared to what we would expect it to be. 
An exponential weighted moving average is formed in order to accomplish this with the following algorithm showing how this average is updated at each round. 

\begin{align*}
\text{If agent is made to fight:} \\
biasAverage \leftarrow (1-e)*biasAverage + e*(1-ratioMadeToFight) \\
\text{Else} \\
biasAverage \leftarrow (1-e)*biasAverage - e*(ratioMadeToFight)
\end{align*}
% \begin{algorithmic}
% \If{$madeToFight$} \\ 
%     \State $biasAverage \gets (1-e)*biasAverage + e*(1-ratioMadeToFight)$ \\
% \Else \\
%     \State  $biasAverage \gets (1-e)*biasAverage - e*(ratioMadeToFight)$ \\
% \EndIf 
% \end{algorithmic}

As can be seen the bias is shifted up or down slightly dependent on whether the agent is made to fight. In order to check that this algorithm was reliable it was tested in python to see how much bias was needed over how many rounds to be detected using this method. In each round of the testing the $ratioMadeToFight$ variable was randomly generated and then multiplied by a constant which was used as the probability that the agent was made to fight. We found that using an $e$ value of $0.1$ over 20 rounds the algorithm correctly predicted that there was a positive bias for a bias multiplier of $1.1$ roughly $80\%$ of the time. Similar results were found for a multiplier of $0.9$. The benefit of this algorithm is that it only requires us to store a single variable. This resultant $biasAverage$ score is positive when there is bad social bias against us and negative when there is favourable social bias.

\section{Forgiveness}

\section{Selfishness}

Starts at a random value, as such we get agents with differing behaviour. Hope is that the social capital framework through sanctions encourages cooperative behaviour and thus low selfishness in the group as a whole.

Weighting of how much agent prioritises its own performance over the performance of the group as a whole.

\subsection{Impact on Decision Making}

\subsection{Updating Selfishness}


\section{Sanctions}

\subsection{Exclusion from Trading}

\subsection{Elections}

Since our agent deign is based on a society using Ostrom style rules it is important to have a mechanism to punish agents that aren't following these rules and acting against the interests of the group. Since the manner in which the rules are implemented does not having any inbuilt sanction system we use the system available of electing a leader as the mechanism though which to impose punishment. When the collective group is generally following the rules our strategy prefers that there is no absolute leader and instead the against vote on rules that should be followed, however, if there we are measuring a great enough amount of negative social capital in the group our agent tries to elect a leader with absolute power in the hope that they will punish the agents working against the groups interests. We can use the social capital scores to decide which agent we most trust to impose this will. 

A problem introduced by this strategy is that when a leader is elected it is expected that they will act against the interests of a a subset of the agents by punishing them. This punishment may also mean that the fight decisions the leader makes may not seem entirely rational from the objective point of view of killing the current monster as fast as possible. These two factors can result in the leader not being liked by a large percentage of the current agents suggesting that being the leader that imposes these sanctions is often not beneficial to an agents long term survival chances. 

The result of this problem is that each agent needs to weigh the negative outcome of no agent taking this responsibility against the cost of being the one to make the decision. It is fairly simple to see that if our agent was the only type of agent in the game, such that every agent in the game had the same strategy, it would be impossible for any one agent to gain the advantage here. So to prevent being stuck in a sub-optimal Nash Equilibrium the system would be devised such that a random agent was chosen each time. However, since the game contains agents from other teams with alternative strategies it may be possible to implement a strategy here that is better than those implemented by other agents. Once again the decision making process our agent is based on finding the number of agents in a similar position to ours and the spread of agents states. We can use this information as well as the social capital of every agent to construct the following manifesto proposal system shown in figure~\ref{team1_manifesto_decision}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{004_team_1_agent_design/images/mainfestostrategy.png}
    \caption{Manifesto proposal algorithm}
    \label{fig:team1_manifesto_decision}
\end{figure}

Each of the decision making steps in the algorithm can be fairly difficult to estimate. We find the potential social reward of enforcing the policy based on how bad the mean current social capital is as well as the spread of agents state. This is because if we have positive social capital for most people then we wont have to introduce many sanctions and thus won't be disliked. Also if the spread of agent states is greater it was found that it can be harder to treat all agents fairly. 

When taking an average of the social the total social capital the mechanism used for taking this average is very important. If we use a general $l_n$ norm the higher the value of $n$ the greater the affect of outliers will be on the computed mean, which may be preferable dependent on our strategy. The $l2$ norm was chosen for our agent but it can be hard to reason what a good choice here should be and so tuning was used to adjust this value. This average is used to decide whether or not to try and vote for a leader.

\section{Q-learning}

\section{Experiments}
do not use HP-pool as it is OP

\section{Agent Performance}

\section{Conclusion}